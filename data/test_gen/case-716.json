{
    "case_id": "case-716",
    "repo_name": "databrickslabs/dolly",
    "file_path": "training/trainer.py",
    "code_snippet": "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n\n    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n\n        batch = super().torch_call(examples)\n\n\n\n        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n\n        # sequence of tokens.  This should just be a single token.\n\n        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n\n\n\n        labels = batch[\"labels\"].clone()\n\n\n\n        for i in range(len(examples)):\n\n\n\n            response_token_ids_start_idx = None\n\n            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n\n                response_token_ids_start_idx = idx\n\n                break\n\n\n\n            if response_token_ids_start_idx is None:\n\n                raise RuntimeError(\n\n                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n\n                )\n\n\n\n            response_token_ids_end_idx = response_token_ids_start_idx + 1\n\n\n\n            # Make pytorch loss function ignore all tokens up through the end of the response key\n\n            labels[i, :response_token_ids_end_idx] = -100\n\n\n\n        batch[\"labels\"] = labels\n\n\n\n        return batch\n",
    "line_range": [
        47,
        77
    ],
    "command_specific_fields": {
        "method_name": "torch_call"
    },
    "language": "python",
    "commit": "e2d664ddf06e0723d818dc56d6c522ead4bd881d",
    "prompt": ""
}