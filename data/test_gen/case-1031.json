{
    "case_id": "case-1031",
    "repo_name": "databrickslabs/dolly",
    "file_path": "training/trainer.py",
    "code_snippet": "\n\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed=DEFAULT_SEED, training_dataset: str = DEFAULT_TRAINING_DATASET) -> Dataset:\n    \"\"\"Loads the training dataset and tokenizes it so it is ready for training.\n\n    Args:\n        tokenizer (AutoTokenizer): Tokenizer tied to the model.\n        max_length (int): Maximum number of tokens to emit from tokenizer.\n\n    Returns:\n        Dataset: HuggingFace dataset\n    \"\"\"\n\n    dataset = load_training_dataset(training_dataset)\n\n    logger.info(\"Preprocessing dataset\")\n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n    )\n\n    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n    logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n    logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n\n    logger.info(\"Shuffling dataset\")\n    dataset = dataset.shuffle(seed=seed)\n\n    logger.info(\"Done preprocessing\")\n\n    return dataset\n",
    "line_range": [
        146,
        179
    ],
    "command_specific_fields": {
        "method_name": "preprocess_dataset"
    },
    "language": "python",
    "commit": "e2d664ddf06e0723d818dc56d6c522ead4bd881d",
    "prompt": ""
}