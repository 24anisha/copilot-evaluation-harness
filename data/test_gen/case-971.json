{
    "case_id": "case-971",
    "repo_name": "databrickslabs/dolly",
    "file_path": "training/trainer.py",
    "code_snippet": "\n\ndef train(\n\n    *,\n\n    input_model: str,\n\n    local_output_dir: str,\n\n    dbfs_output_dir: str,\n\n    epochs: int,\n\n    per_device_train_batch_size: int,\n\n    per_device_eval_batch_size: int,\n\n    lr: float,\n\n    seed: int,\n\n    deepspeed: str,\n\n    gradient_checkpointing: bool,\n\n    local_rank: str,\n\n    bf16: bool,\n\n    logging_steps: int,\n\n    save_steps: int,\n\n    eval_steps: int,\n\n    test_size: Union[float, int],\n\n    save_total_limit: int,\n\n    warmup_steps: int,\n\n    training_dataset: str = DEFAULT_TRAINING_DATASET,\n\n):\n\n    set_seed(seed)\n\n\n\n    model, tokenizer = get_model_tokenizer(\n\n        pretrained_model_name_or_path=input_model, gradient_checkpointing=gradient_checkpointing\n\n    )\n\n\n\n    # Use the same max length that the model supports.  Fall back to 1024 if the setting can't be found.\n\n    # The configuraton for the length can be stored under different names depending on the model.  Here we attempt\n\n    # a few possible names we've encountered.\n\n    conf = model.config\n\n    max_length = None\n\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n\n        max_length = getattr(model.config, length_setting, None)\n\n        if max_length:\n\n            logger.info(f\"Found max lenth: {max_length}\")\n\n            break\n\n    if not max_length:\n\n        max_length = 1024\n\n        logger.info(f\"Using default max length: {max_length}\")\n\n\n\n    processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length, seed=seed, training_dataset=training_dataset)\n\n\n\n    split_dataset = processed_dataset.train_test_split(test_size=test_size, seed=seed)\n\n\n\n    logger.info(\"Train data size: %d\", split_dataset[\"train\"].num_rows)\n\n    logger.info(\"Test data size: %d\", split_dataset[\"test\"].num_rows)\n\n\n\n    data_collator = DataCollatorForCompletionOnlyLM(\n\n        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n\n    )\n\n\n\n    # enable fp16 if not bf16\n\n    fp16 = not bf16\n\n\n\n    if not dbfs_output_dir:\n\n        logger.warn(\"Will NOT save to DBFS\")\n\n\n\n    training_args = TrainingArguments(\n\n        output_dir=local_output_dir,\n\n        per_device_train_batch_size=per_device_train_batch_size,\n\n        per_device_eval_batch_size=per_device_eval_batch_size,\n\n        fp16=fp16,\n\n        bf16=bf16,\n\n        learning_rate=lr,\n\n        num_train_epochs=epochs,\n\n        deepspeed=deepspeed,\n\n        gradient_checkpointing=gradient_checkpointing,\n\n        logging_dir=f\"{local_output_dir}/runs\",\n\n        logging_strategy=\"steps\",\n\n        logging_steps=logging_steps,\n\n        evaluation_strategy=\"steps\",\n\n        eval_steps=eval_steps,\n\n        save_strategy=\"steps\",\n\n        save_steps=save_steps,\n\n        save_total_limit=save_total_limit,\n\n        load_best_model_at_end=False,\n\n        report_to=\"tensorboard\",\n\n        disable_tqdm=True,\n\n        remove_unused_columns=False,\n\n        local_rank=local_rank,\n\n        warmup_steps=warmup_steps,\n\n    )\n\n\n\n    logger.info(\"Instantiating Trainer\")\n\n\n\n    trainer = Trainer(\n\n        model=model,\n\n        tokenizer=tokenizer,\n\n        args=training_args,\n\n        train_dataset=split_dataset[\"train\"],\n\n        eval_dataset=split_dataset[\"test\"],\n\n        data_collator=data_collator,\n\n    )\n\n\n\n    logger.info(\"Training\")\n\n    trainer.train()\n\n\n\n    logger.info(f\"Saving Model to {local_output_dir}\")\n\n    trainer.save_model(output_dir=local_output_dir)\n\n\n\n    if dbfs_output_dir:\n\n        logger.info(f\"Saving Model to {dbfs_output_dir}\")\n\n        trainer.save_model(output_dir=dbfs_output_dir)\n\n\n\n    logger.info(\"Done.\")\n",
    "line_range": [
        180,
        288
    ],
    "command_specific_fields": {
        "method_name": "train"
    },
    "language": "python",
    "commit": "e2d664ddf06e0723d818dc56d6c522ead4bd881d",
    "prompt": ""
}