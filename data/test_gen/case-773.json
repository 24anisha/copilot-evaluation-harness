{
    "case_id": "case-773",
    "repo_name": "facebookresearch/detr",
    "file_path": "models/transformer.py",
    "code_snippet": "\n\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n\n                 activation=\"relu\", normalize_before=False):\n\n        super().__init__()\n\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n\n        # Implementation of Feedforward model\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n\n        self.dropout = nn.Dropout(dropout)\n\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n\n\n        self.norm1 = nn.LayerNorm(d_model)\n\n        self.norm2 = nn.LayerNorm(d_model)\n\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.dropout2 = nn.Dropout(dropout)\n\n\n\n        self.activation = _get_activation_fn(activation)\n\n        self.normalize_before = normalize_before\n",
    "line_range": [
        127,
        144
    ],
    "command_specific_fields": {
        "method_name": "__init__"
    },
    "language": "python",
    "commit": "29901c51d7fe8712168b8d0d64351170bc0f83e0",
    "prompt": ""
}