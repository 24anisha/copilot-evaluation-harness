{
    "case_id": "case-1114",
    "repo_name": "databrickslabs/dolly",
    "file_path": "training/generate.py",
    "code_snippet": "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n\n\n\n    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_full_text: bool = False):\n\n\n\n        generated_sequence = model_outputs[\"generated_sequence\"][0]\n\n        instruction_text = model_outputs[\"instruction_text\"]\n\n\n\n        generated_sequence: List[List[int]] = generated_sequence.numpy().tolist()\n\n        records = []\n\n        for sequence in generated_sequence:\n\n\n\n            # The response will be set to this variable if we can identify it.\n\n            decoded = None\n\n\n\n            # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n\n            if response_key_token_id and end_key_token_id:\n\n                # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n\n                # prompt, we should definitely find it.  We will return the tokens found after this token.\n\n                try:\n\n                    response_pos = sequence.index(response_key_token_id)\n\n                except ValueError:\n\n                    logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n\n                    response_pos = None\n\n\n\n                if response_pos:\n\n                    # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n\n                    # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n\n                    # this token, as the response could be truncated.  If we don't find it then just return everything\n\n                    # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n\n                    try:\n\n                        end_pos = sequence.index(end_key_token_id)\n\n                    except ValueError:\n\n                        end_pos = None\n\n\n\n                    decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n\n\n\n            if not decoded:\n\n                # Otherwise we'll decode everything and use a regex to find the response and end.\n\n\n\n                fully_decoded = self.tokenizer.decode(sequence)\n\n\n\n                # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n\n                # end.\n\n                m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n\n\n\n                if m:\n\n                    decoded = m.group(1).strip()\n\n                else:\n\n                    # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n\n                    # return everything after \"### Response:\".\n\n                    m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n\n                    if m:\n\n                        decoded = m.group(1).strip()\n\n                    else:\n\n                        logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n\n\n\n            # If the full text is requested, then append the decoded text to the original instruction.\n\n            # This technically isn't the full text, as we format the instruction in the prompt the model has been\n\n            # trained on, but to the client it will appear to be the full text.\n\n            if return_full_text:\n\n                decoded = f\"{instruction_text}\\n{decoded}\"\n\n\n\n            rec = {\"generated_text\": decoded}\n\n\n\n            records.append(rec)\n\n\n\n        return records\n",
    "line_range": [
        151,
        217
    ],
    "command_specific_fields": {
        "method_name": "postprocess"
    },
    "language": "python",
    "commit": "e2d664ddf06e0723d818dc56d6c522ead4bd881d",
    "prompt": ""
}