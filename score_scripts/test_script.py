# Copyright (c) Microsoft. All rights reserved.
"""Evaluating Copilot for /test"""

import re
from pathlib import Path
from contextlib import contextmanager
from typing import List, Tuple, Optional
import json
from plum.environments.py_repo import PythonRepository
from plum.environments.js_repo import JavascriptRepository
from plum.actions.js_actions import JavascriptActions
from plum.actions.py_actions import PythonActions
from plum.environments.java_repo import JavaRepository
from plum.actions.java_mvn_actions import JavaMavenActions
from plum.environments.csharp_repo import CsharpRepository
from plum.actions.csharp_dotnet_actions import CsharpDotnetActions
from plum_exec_tools import setup_js_ts_repo_for_test_generation
from base_evaluator import EvaluationResult, Evaluator
from timeout import timeout


def get_code_from_outcome(outcome: dict, language: str) -> Optional[str]:
    """
    Extracts the first code snippet from a markdown string.

    Args:
        markdown (str): The markdown string containing code snippets.
        language (str): The language of the code snippet to extract.

    Returns:
        str: The extracted code snippet, otherwise the original string if no code snippet is found
    """

    extracted_text = completion = outcome["response"]
    start_marker = f"```{language}"
    try:
        start_index = completion.find(start_marker)
    except:
        return None

    if start_index == -1:
        start_marker = "```"
        start_index = completion.find(start_marker)
    end_marker = "\n```"
    end_index = completion.rfind(end_marker)
    if start_index != -1 and end_index != -1:
        extracted_text = completion[start_index + len(start_marker) : end_index]

    # remove the import line so I can add in my own imports
    if language in ("javascript", "typescript"):
        lines = extracted_text.split("\n")
        filtered_lines = [line for line in lines if not line.startswith("import ") and "require(" not in line]  # what if someone uses require as part of a var name?
        extracted_text = "\n".join(filtered_lines)

    return extracted_text


@contextmanager
def temporary_file_content_change(filename, temporary_content):
    """Context manager to temporarily change the contents of a file"""
    with open(filename, "r") as fin:
        original_file_content = fin.read()

    try:
        with open(filename, "w") as fout1:
            fout1.write(temporary_content)
        yield
    finally:
        with open(filename, "w") as fout2:
            fout2.write(original_file_content)


class TestGenEvaluator(Evaluator):
    """Evaluate the quality of python test generation"""

    def __init__(self, **kwargs):
        super().__init__("test_gen", **kwargs)

    def modify_import(self, code: str, import_statement: str, name: str) -> str:
        """Find the line with both "import" and "name" and replace it with import_statment"""
        lines = code.split("\n")

        # TODO this does not handle multiple import syntax in python e.g., import (a, b) which can be split across lines
        for i, line in enumerate(lines):
            if "import" in line and name in line:
                lines[i] = import_statement
                break
        return "\n".join(lines)

    def get_class_name_java_csharp(self, response):
        """
        Get the class name from the generated test
        """
        # Search for the pattern "public class [ClassName]"
        class_name_match = re.search(r"public class (\S+)", response)
        if class_name_match:
            class_name = class_name_match.group(1)
        else:
            class_name = "ClassName"
        return class_name

    # TODO: the evaluation for each language needs to be in its own function; then remove the pylint disables
    # pylint: disable=too-many-arguments, too-many-branches, too-many-statements
    @timeout(600)
    def evaluate_generated_test(
        self,
        language: str,
        generated_test: str,
        base_path: Path,
        repo_folder_name: str,
        relative_path: Path,
        conversation,
        case_number,
    ) -> Tuple[bool, str, str, str]:
        """Evaluate a generated test from Copilot using PLUM

        Args:
            generated_test (str): the string of the test code generated by Copilot
            fn_name (str): the name of the function being tested
            fn_hash (str): the hash of the function being tested
            repo_folder_name (str): the name of the repo folder (not including any other parts of the path)

        Returns:
            Tuple[bool, str, str]: the test result (pass or fail), stdout and stderr"""
        print(f"Evaluating generated test for {language} in {base_path} with {repo_folder_name}")
        failure_reason = None
        test_contents_used = generated_test
        if language == "python":
            repo = PythonRepository(base_path, repo_folder_name)
            repo.setup()
            actions = PythonActions(repo)
            test_file = actions.save_generated_test(generated_test, "CES_generated")
            result = actions.execute_test_file(test_file, timeout=60)

            if "status_result" in result and "success" not in result:
                result["success"] = result["status_result"] == "SUCCESS"
            if "success" not in result:
                result["success"] = False

            repo.cleanup()
        elif language in ("javascript", "typescript"):
            repo = JavascriptRepository(base_path, repo_folder_name, language=language)
            repo.setup(install_reqs=True)
            actions = JavascriptActions(repo)
            test_library = repo.test_library or "jest"
            setup_js_ts_repo_for_test_generation(repo, test_library, language)
            focal_file_path = base_path / Path(repo.internal_repo_path) / Path(relative_path)

            if test_library == "mocha":
                imports_by_language = {
                    "javascript": "var chai = require('chai')\nvar assert = chai.assert;\nvar expect = chai.expect;",
                    "typescript": "import { assert, expect, should } from 'chai'\n",
                }
                imports = imports_by_language[language]
                test_contents_used = f"{imports}\n\n{focal_file_path.read_text()}\n\n{generated_test}"

                # use context manager to replace the contents of the focal file with edited prompt
                with temporary_file_content_change(str(focal_file_path), test_contents_used):
                    result = actions.run_npm_test(relative_path, test_library, timeout=60)
            else:
                # If jest, name of the file has to be changes to filename.test.js

                test_contents_used = f"{focal_file_path.read_text()}\n\n{generated_test}"
                # TODO once PLUM does the .test. conversion itself we won't need to do this anymore
                test_file_name = relative_path.stem + "_CES_Generated_Test.test" + relative_path.suffix
                test_file_path = base_path / Path(repo.internal_repo_path) / relative_path.parent / Path(test_file_name)

                test_file_path.write_text(test_contents_used)
                try:
                    result = actions.run_npm_test(test_file_path.relative_to(base_path), test_library, timeout=60)
                finally:
                    test_file_path.unlink()

            if "status_result" in result and "success" not in result:
                result["success"] = result["status_result"] == "SUCCESS"
            if "success" not in result:
                result["success"] = False

            # TODO: delete this after reducing the number of test failures
            if not result.get("success"):
                matches = [
                    # re.search(r"(ReferenceError): \w+ is not defined", result["stdout"], flags=re.MULTILINE),
                    # re.search(r"(AssertionError[^\n]+)", result["stdout"], flags=re.MULTILINE),
                    # re.search(r"(TypeError[^\n]+)", result["stderr"], flags=re.MULTILINE),
                    # re.search(r"\s+(\w+Error:[^\n]+)", result["stderr"], flags=re.MULTILINE),
                    # re.search(r"\s+(\w+Error:[^\n]+)", result["stdout"], flags=re.MULTILINE),
                    re.search(r"\s+(\w+Error):", result["stderr"], flags=re.MULTILINE),
                    re.search(r"\s+(\w+Error):", result["stdout"], flags=re.MULTILINE),
                ]
                for match in matches:
                    if match and not failure_reason:
                        failure_reason = match.group(1)
                        break

            # If the test passes, we want to check if the test was already passing
            # If the test was already passing, we should return False
            # If the test was not already passing, we should return True
            # This is because we want to reward the model for generating a test that is not already passing
            # This is a temporary solution until we can get the before/after state of the repo

            # call repo.cleanup(), but currently it deletes the entire repo directory which makes debugging difficult
            repo.cleanup()

        elif language == "java":
            repo = JavaRepository(base_path, repo_folder_name)
            repo.setup(cleanup=False, install_reqs=True)

            # PLUM Actions
            docker_image = "maven"
            docker_tag = "3.3-jdk-8"
            plum = JavaMavenActions(repo, docker_image, docker_tag)

            class_name = self.get_class_name_java_csharp(generated_test)

            # Custom command for Apache repos
            if "apache" in repo_folder_name:
                print("Running spotless:apply")
                result = plum.run_custom_command("mvn spotless:apply")
                # TODO: should we check the result of this command?

            result = plum.run_generated_test(generated_test)
            if "status_result" in result and "success" not in result:
                result["success"] = result["status_result"] == "SUCCESS"
            if "success" not in result:
                result["success"] = False

            print("run_generated_test result:", result)
            test_contents_used = result.pop("test_contents_used", generated_test)
            if "src/test/java doesn't exist" in result.get("stdout", "") \
                or result.get("stderr", "").startswith("Cannot parse structure"):
                failure_reason = "src/test/java doesn't exist"
                failure_reason = "src/test/java doesn't exist"
            if re.search(r"package \S+ does not exist", result.get("stdout", ""), re.MULTILINE):
                failure_reason = "package <package name> does not exist"

            repo.cleanup()

        elif language == "csharp":
            repo = CsharpRepository(base_path, repo_folder_name)
            repo.setup(cleanup=False, install_reqs=True)

            # PLUM Actions
            docker_image = "mcr.microsoft.com/dotnet/sdk"
            docker_tag = "6.0"
            plum = CsharpDotnetActions(repo, docker_image, docker_tag)

            # Extract response
            class_name = self.get_class_name_java_csharp(generated_test)
            result = {}

            def get_test_project_path():
                test_projects = plum.get_test_projects()
                if not test_projects:
                    raise FileNotFoundError("No test projects found for test generation.")

                # narrow the list of test projects to the type required by the conversation
                def get_test_project_type(value: str):
                    for pkg in ['nunit', 'xunit']:
                        if pkg in value.casefold():
                            return pkg
                    return None

                conversation_question = conversation[0].get("question", "")
                conversation_test_project_type = get_test_project_type(conversation_question)
                if not conversation_test_project_type:
                    raise ValueError("No test project type found in conversation.")
                matching_test_projects = [p for p in test_projects if p.get_test_project_type().casefold() == conversation_test_project_type]
                if not matching_test_projects:
                    raise FileNotFoundError(f"No test projects found for test generation of type {conversation_test_project_type} "\
                                            f"in the conversation from candidate test projects {test_projects} "\
                                            f"{[p.get_test_project_type() for p in test_projects]} from question {conversation_question}.")
                if len(matching_test_projects) > 1:
                    print(f"WARNING: Multiple test projects found for test generation. Using first file: {[p.csproj_path for p in matching_test_projects]}")
                test_project = matching_test_projects[0]
                test_project_path: Path = Path(test_project.csproj_path)
                test_folder: Path = test_project_path.parent
                return test_folder

            test_folder = get_test_project_path()
            new_test_file = test_folder / (class_name + ".cs")
            new_test_file.write_text(test_contents_used)

            # Compile
            result = plum.build()

            if "status_result" in result and "success" not in result:
                result["success"] = result["status_result"] == "SUCCESS"
            if "success" not in result:
                result["success"] = False

            # TODO: call plum.run_test() if the build succeeds and the generated tests passes

            # repo.cleanup() deletes the entire repo directory which makes debugging difficult, so comment out this line when using local execution
            repo.cleanup()

        elif language == "cpp":
            # TODO: it is unclear where to place cpp tests because the testing framework may look for test cases arbitrarily.
            # For now, we will rely on syntax correctness (checked as the last step of eval, so we pass this through as correct)

            # TODO: this is not a real evaluation, do a real evaluation instead of returning success; for now,
            #       evaluate_test_case will turn this into a no_score value so once this does a real evaluation,
            #       evaluate_test_case will need to be updated to handle the new evaluation

            # repo.cleanup() deletes the entire repo directory which makes debugging difficult, so comment out this line when using local execution
            # repo.cleanup()

            result = {
                "success": True,
                "stdout": "Relying on syntax check",
                "stderr": "Relying on syntax check",
            }

        # Save the generated test to a file in the output directory for diagnostic purposes
        if self.evaluator_output_path:
            ext_map = {
                "python": "py",
                "javascript": "js",
                "typescript": "ts",
                "java": "java",
                "csharp": "cs",
                "cpp": "cpp",
            }
            generated_test_ext = ext_map.get(language, ".txt")
            generated_tests_root_path = self.evaluator_output_path / "generated_tests"
            generated_tests_root_path.mkdir(parents=True, exist_ok=True)
            generated_test_fname = generated_tests_root_path / f"case-{case_number}_generated_test.{generated_test_ext}"
            with open(generated_test_fname, "wt", encoding="utf-8") as generated_test_file:
                generated_test_file.write(test_contents_used)

        return result["success"], result["stdout"], result["stderr"], failure_reason

    def evaluate_test_case(
        self,
        working_dir: Path,
        _: Path,
        case_number: str,
        test_case_id: str,
        n_id: str,
        state: dict,
        conversation: dict,
        language: str,
        document_abs_path: Path,
        outcome: dict,
        ___: List[Path],
    ) -> List[EvaluationResult]:
        """Scores each bug based on whether Copilot generated a valid workspace."""
        results: List[EvaluationResult] = []
        generated_test = get_code_from_outcome(outcome, language)
        tested_method_name = conversation[0].get("original_method_name", None)
        failure_reasons = []
        if generated_test is None:
            score = 0
            failure_reasons.append("No test was generated from the chat response")
            stderr = ""
            stdout = ""
        elif tested_method_name is None:
            score = 0
            failure_reasons.append("No original method name found in conversation - are you sure your test cases are up to date?")
            stderr = ""
            stdout = ""
        elif tested_method_name not in generated_test and not (
            tested_method_name in ("constructor", "call", "init") or (tested_method_name.startswith("__") and tested_method_name.endswith("__"))
        ):
            # The second condition of this case prevents the following corner cases from being included:
            # We do not check tested_method_name presence for __call__, __init__ and other constructors will not show the method or class name in the tests
            # TODO also exclude routes like testing APIs using /users, etc. will not show the method or class name in the tests (but may include the name in the test method name)
            score = 0
            failure_reasons.append("Tested method does not appear in the generated test")
            print("TESTED METHOD:", tested_method_name)
            print("GENERATED TEST:")
            print(generated_test)
            print("OUTCOME:")
            print(outcome)
            stderr = ""
            stdout = ""
        else:
            # Evaluate the generated test
            success, stdout, stderr, failure_reason = self.evaluate_generated_test(
                language,
                generated_test,
                str(working_dir),
                "",
                Path(state["activeTextEditor"]["documentFilePath"]),
                conversation,
                case_number,
            )
            # The score is 0 on failure with the assumption that the project built before the test was added.
            # TODO: when cpp actually does an evaluation, then the conditional for it can be removed.
            score = int(success) if not language=="cpp" else Evaluator.no_score
            if not success:
                failure_reasons.append(failure_reason if failure_reason else "Test failed")

        syntax_is_correct = self.syntax_parser.file_contents_syntax_check(generated_test, language)
        if not syntax_is_correct:
            score = 0
            failure_reasons.append("Syntax error in generated test")

        results.append(
            EvaluationResult(
                self.score_name,
                case_number,
                test_case_id,
                n_id,
                language,
                score,
                ",".join(failure_reasons),
                self.syntax_parser.check_syntax_by_file([document_abs_path], language),
                {
                    "generated_test": self.syntax_parser.file_contents_syntax_check(generated_test, language),
                },
                json.dumps({
                    "response": outcome["response"],
                    "generated_test": generated_test,
                    "original_method": conversation[0].get("original_method", None),
                    "prompt": conversation[0]["question"],
                    "stderr": stderr,
                    "stdout": stdout,
                }),
            )
        )
        return results
