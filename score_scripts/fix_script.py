# Copyright (c) Microsoft. All rights reserved.
"""Evaluating Copilot for fixing bugs in source code."""

import os
import pandas as pd
import re
import json
import dataclasses
import difflib
from pathlib import Path
from typing import List, Tuple

from abs_conversation import get_task
from abs_state import get_document_file_path, get_terminal_output, get_active_line, get_anchor_line
from base_evaluator import EvaluationResult, Evaluator
from static_analysis_tools import (
    ToolResult,
    get_supported_tools,
    get_tool_run_fn,
)
from timeout import timeout


class FixEvaluator(Evaluator):
    """Evaluate the quality of bug fixes generated for a dataset of code files."""

    def __init__(self, **kwargs):
        super().__init__("fix", **kwargs)
        self.before_fix_evaluation = {}

    def _evaluate_logical_fix(self, proposals: pd.Series, tests: pd.Series) -> List[int]:
        """
        Extract scores from responses via execution.

        Args:
            responses (List[str]): The responses.
        Returns:
            List[int]: The execution pass/fail scores.
        """
        scores = []
        patterns = [
            r"```python\n(.*?)```",
            r"python\n?```\n?(.*?)```",
        ]
        regex_patterns = [re.compile(pattern, re.DOTALL) for pattern in patterns]

        for proposal, test in zip(proposals, tests):
            matches = [reg.search(proposal) for reg in regex_patterns]
            matches = [match for match in matches if match is not None]
            if len(matches) == 0:
                scores.append(0)
                continue

            proposed_code = sorted(matches, key=lambda m: m.start())[0].group(1)
            executable_test = f"{proposed_code}\n\n{test}\n\ntest()"
            try:
                exec(executable_test, globals())  # pylint: disable=exec-used
                scores.append(1)
            except Exception:  # pylint: disable=broad-exception-caught
                scores.append(0)
        return scores

    def evaluate_fix_with_tool(
        self,
        tool: str,
        repo_folder: Path,
        before_file_path: Path,
        after_file_contents: str,
    ) -> Tuple[float, str, List[ToolResult], List[ToolResult]]:
        """Evaluate the quality of a fix generated by Copilot.

        Args:
            tool (str): The tool to use for evaluation, e.g. pylint, pyright, tsc

        Returns:
            Tuple[float, str, list, list]: A tuple containing the score, reason, and the before and after errors.
        """

        before_file_contents = before_file_path.read_text()
        after_results = []
        before_results = []
        tool_raw_output = []

        tool_fn = get_tool_run_fn(tool)

        cache_key = (str(repo_folder), tool)
        if cache_key in self.before_fix_evaluation:
            before_results = self.before_fix_evaluation[cache_key]
            print("USED CACHE KEY")
        else:
            before_results, _ = tool_fn(repo_folder)
            self.before_fix_evaluation[cache_key] = before_results

        try:
            before_file_path.write_text(after_file_contents)
            after_results, tool_raw_output = tool_fn(repo_folder)

            # There should never be a case of missing before results, so if it is None or an empty list, then there was a tool failure.
            # We can't tell the difference between a tool failure and all problems fixed for after_results. But since that check comes
            # after the check for before_results, it shouldn't cause an issue in practice even though in theory, it could.
            if not before_results:
                score = 0
                reason = f"{tool} failed to run on before fix file"
            elif after_results is None:
                score = 0
                reason = f"{tool} failed to run on after fix file"
            elif len(before_results) > len(after_results):
                score = 1
                reason = f"({tool}) After fix has fewer errors"
            elif len(before_results) == len(after_results):
                score = 0
                reason = f"({tool}) After fix has the same # of errors"
            else:
                score = 0
                reason = f"({tool}) After fix has more errors"
        except:
            score = 0
            reason = f"({tool}) After fix failed to run"
        finally:
            before_file_path.write_text(before_file_contents)

        return score, reason, before_results or [], after_results or [tool_raw_output]

    # @override
    # pylint: disable=too-many-arguments
    @timeout(600)
    def evaluate_test_case(
        self,
        working_dir: Path,  # this is the workspace folder
        _: Path,
        case_number: int,
        test_case_id,
        n_id: str,
        state: dict,
        conversation: dict,
        language: str,
        document_abs_path: Path,
        outcome: List[dict],
        post_files: List[Path],
    ) -> List[EvaluationResult]:
        """Scores each bug based on whether Copilot generated a valid fix."""

        if len(post_files) == 0:
            return [self.get_missing_post_file_eval_result(case_number, n_id, language, test_case_id)]

        if len(post_files) > 1:
            print(f"TOO MANY POST FILES DETECTED for case {case_number}: Expected 1 but got {len(post_files)}")
            print(post_files)

        post_file_path = post_files[0]
        task = get_task(conversation)
        document_file_path = get_document_file_path(state)
        input_source_file_path = working_dir / document_file_path
        input_source_file_contents = input_source_file_path.read_text()
        output_source_file_contents = post_file_path.read_text()
        unidiff = "\n".join(
            list(
                difflib.unified_diff(
                    input_source_file_contents.splitlines(),
                    output_source_file_contents.splitlines(),
                    fromfile="before",
                    tofile="after",
                )
            )
        )

        if task in get_supported_tools():
            (
                score,
                reason,
                before_errors,
                after_errors,
            ) = self.evaluate_fix_with_tool(
                task,
                working_dir,
                input_source_file_path,
                output_source_file_contents,
            )
        else:
            score, reason, before_errors, after_errors = (
                Evaluator.no_score,
                "Task not supported",
                [],
                [],
            )
        original_err = (
            get_terminal_output(state),
            get_active_line(state),
            get_anchor_line(state),
        )
        simplified_before_errs = [(err.message, err.start_line_index, err.end_line_index) for err in before_errors]

        if not isinstance(after_errors[0], ToolResult):
            errors_only_in_before = [dataclasses.asdict(err) for err in before_errors]
            errors_only_in_after = after_errors
            original_err_is_in_after_analysis = None
            extra_errors_after = after_errors
        else:
            simplified_after_errs = [(err.message, err.start_line_index, err.end_line_index) for err in after_errors]
            errors_only_in_before = [dataclasses.asdict(err) for err in before_errors if (err.message, err.start_line_index, err.end_line_index) not in simplified_after_errs]
            errors_only_in_after = [dataclasses.asdict(err) for err in after_errors if (err.message, err.start_line_index, err.end_line_index) not in simplified_before_errs]
            original_err_is_in_after_analysis = any(err for err in simplified_after_errs if err == original_err)
            extra_errors_after = [dataclasses.asdict(err) for err in after_errors]

        failure_reasons = [reason]
        if not self.syntax_parser.file_contents_syntax_check(output_source_file_contents, language):
            score = 0
            failure_reasons.append("Syntax error in generated code")

        original_file_syntax_pass = self.syntax_parser.check_syntax_by_file([working_dir / document_file_path], language)
        post_file_syntax_pass = self.syntax_parser.check_syntax_by_file(post_files, language)

        # this tracing runs on local VM only; it's only for debugging
        if os.environ.get("AZUREML_WORKSPACE_SCOPE", "local") == "local":
            trace_data = {
                "score_name": self.score_name,
                "case_number": case_number,
                "test_case_id": test_case_id,
                "n_id": n_id,
                "language": language,
                "score": score,
                "failure_reasons": failure_reasons,
                "original_file_syntax_pass": original_file_syntax_pass,
                "post_file_syntax_pass": post_file_syntax_pass,
                "source_file_path": str(input_source_file_path),
                "original_error_is_in_after_analysis": original_err_is_in_after_analysis,
                "original_error": original_err,
                "unidiff": unidiff,
                "outcome": outcome,
                "errors_only_in_before": errors_only_in_before,
                "errors_only_in_after": errors_only_in_after,
                "errors_before": [dataclasses.asdict(err) for err in before_errors],
                "errors_after": extra_errors_after,
                "task": task,
            }
            trace_path = self.output_path / "test_case_traces" / f"{case_number}-{n_id}"
            trace_path.mkdir(parents=True, exist_ok=True)
            with open(trace_path / "evaluation_result", "wt") as trace_file:
                json.dump(trace_data, trace_file, indent=4)
            with open(trace_path / "errors_before.json", "wt") as trace_file:
                json.dump([dataclasses.asdict(e) for e in before_errors], trace_file, indent=4)
            if isinstance(after_errors[0], ToolResult):
                with open(trace_path / "errors_after.json", "wt") as trace_file:
                    json.dump([dataclasses.asdict(e) for e in after_errors], trace_file, indent=4)
            else:
                with open(trace_path / "errors_after.log", "wt") as trace_file:
                    trace_file.write(str(after_errors[0]))

        return [
            EvaluationResult(
                self.score_name,
                case_number,
                test_case_id,
                n_id,
                language,
                score,
                ",".join(failure_reasons),
                original_file_syntax_pass,
                post_file_syntax_pass,
                json.dumps(
                    {
                        "source_file_path": str(input_source_file_path),
                        "original_error_is_in_after_analysis": original_err_is_in_after_analysis,
                        "unidiff": unidiff,
                        "outcome": outcome,
                        "original_error": original_err,
                        "errors_only_in_before": errors_only_in_before,
                        "errors_only_in_after": errors_only_in_after,
                        "errors_before": [dataclasses.asdict(err) for err in before_errors],
                        "errors_after": extra_errors_after,
                        "task": task,
                    }
                ),
            )
        ]
