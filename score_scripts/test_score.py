import re
import json
import os
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
from plum.environments import Repository
from plum.environments.js_repo import JavascriptRepository
from plum.actions import Actions
import datetime

OUTPUT_DIR = "out"
REPOS_DIR = os.path.join(OUTPUT_DIR, "repos")
RESULTS_DIR = os.path.join(OUTPUT_DIR, "results")

def get_code_from_outcome(outcome: str, language: str) -> Optional[str]:
    start_marker = f"```{language}"
    end_marker = "```"
    
    start_index = outcome.find(start_marker)
    if start_index == -1:
        start_marker = "```"
        start_index = outcome.find(start_marker)
    
    if start_index == -1:
        return None
    
    end_index = outcome.find(end_marker, start_index + len(start_marker))
    if end_index == -1:
        return None
    
    extracted_text = outcome[start_index + len(start_marker):end_index].strip()
    
    if language in ("javascript", "typescript"):
        lines = extracted_text.split("\n")
        filtered_lines = [line for line in lines if not line.startswith("import ") and "require(" not in line]
        extracted_text = "\n".join(filtered_lines)
    
    return extracted_text

def temporary_file_content_change(filename, temporary_content):
    """Context manager to temporarily change the contents of a file"""
    with open(filename, "r") as fin:
        original_file_content = fin.read()

    try:
        with open(filename, "w") as fout1:
            fout1.write(temporary_content)
        yield
    finally:
        with open(filename, "w") as fout2:
            fout2.write(original_file_content)


def setup_js_ts_repo_for_test_generation(repo: JavascriptRepository, test_library: str, language: str) -> None:
    """Modify javascript/typescript repo in place for test generation"""

    assert language in (
        "javascript",
        "typescript",
    ), f"Expected javascript or typescript but got {language}"
    if language == "javascript":
        repo.overwrite_package_json(command=test_library)
    elif language == "typescript":
        repo.overwrite_package_json(keys=("scripts", "lint"), command="")
        repo.overwrite_package_json(keys=("scripts", "build"), command="tsc")
        if test_library == "mocha":
            repo.overwrite_package_json(command="mocha -r ts-node/register")
        else:
            repo.overwrite_package_json(command="jest")

def evaluate_generated_test(
    language: str,
    generated_test: str,
    base_path: Path,
    repo_folder_name: str,
    relative_path: Path
) -> Tuple[bool, str, str, Optional[str]]:
    """Evaluate a generated test from Copilot using PLUM

    Args:
        generated_test (str): the string of the test code generated by Copilot
        fn_name (str): the name of the function being tested
        fn_hash (str): the hash of the function being tested
        repo_folder_name (str): the name of the repo folder (not including any other parts of the path)

    Returns:
        Tuple[bool, str, str]: the test result (pass or fail), stdout and stderr"""
    print(f"Evaluating generated test for {language} in {base_path} with {repo_folder_name}")
    failure_reason = None
    test_contents_used = generated_test

    if language == "python":
        repo = Repository("python", base_path, repo_folder_name)
        repo.setup(cleanup=False)
        actions = Actions("python", repo)
        test_file_path = repo.repo_root / "CES_generated.py"
        result = actions.run_generated_test(generated_test, test_file_path)

        if "status_result" in result and "success" not in result:
            result["success"] = result["status_result"] == "SUCCESS"
        if "success" not in result:
            result["success"] = False

        repo.cleanup()
    elif language in ("javascript", "typescript"):
        repo = Repository(language, base_path, repo_path=repo_folder_name)
        repo.setup(install_reqs=True, cleanup=False)
        actions = Actions(language, repo)
        test_library = repo.test_library or "jest"
        setup_js_ts_repo_for_test_generation(repo, test_library, language)
        focal_file_path = base_path / Path(repo.internal_repo_path) / Path(relative_path)

        if test_library == "mocha":
            imports_by_language = {
                "javascript": "var chai = require('chai')\nvar assert = chai.assert;\nvar expect = chai.expect;",
                "typescript": "import { assert, expect, should } from 'chai'\n",
            }
            imports = imports_by_language[language]
            test_contents_used = f"{imports}\n\n{focal_file_path.read_text()}\n\n{generated_test}"

            # use context manager to replace the contents of the focal file with edited prompt
            with temporary_file_content_change(str(focal_file_path), test_contents_used):
                result = actions.run_npm_test(relative_path, test_library, timeout=60)
        else:
            # If jest, name of the file has to be changes to filename.test.js

            test_contents_used = f"{focal_file_path.read_text()}\n\n{generated_test}"
            # TODO once PLUM does the .test. conversion itself we won't need to do this anymore
            test_file_name = relative_path.stem + "_CES_Generated_Test.test" + relative_path.suffix
            test_file_path = base_path / Path(repo.internal_repo_path) / relative_path.parent / Path(test_file_name)

            test_file_path.write_text(test_contents_used)
            try:
                result = actions.run_npm_test(test_file_path.relative_to(base_path), test_library, timeout=60)
            finally:
                test_file_path.unlink()

        if "status_result" in result and "success" not in result:
            result["success"] = result["status_result"] == "SUCCESS"
        if "success" not in result:
            result["success"] = False

        # TODO: delete this after reducing the number of test failures
        if not result.get("success"):
            matches = [
                re.search(r"\s+(\w+Error):", result["stderr"], flags=re.MULTILINE),
                re.search(r"\s+(\w+Error):", result["stdout"], flags=re.MULTILINE),
            ]
            for match in matches:
                if match and not failure_reason:
                    failure_reason = match.group(1)
                    break
        repo.cleanup()
    return result.get("success", False), result.get("stdout", ""), result.get("stderr", ""), failure_reason

def score_test(base_path: Path, repo_folder_name: str, relative_path: Path, language: str, model_response: str, case_id: str) -> Dict[str, Any]:
    generated_test = get_code_from_outcome(model_response, language)

    file_type = {"python": ".py", "java": ".java", "javascript": ".js", "typescript": ".ts", "csharp": ".cs"}[language]

    out_dir = os.path.join(RESULTS_DIR, f"test_gen_{datetime.date.today()}", f"{case_id}")
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    with open(os.path.join(out_dir, f"after_contents{file_type}"), 'w') as f:
        f.write(generated_test)

    if generated_test is None:
        return {
        "metric": "test",
        "success": False,
        "score": 0,
        "language": language,
        "reason": "No test code in model response",
        "extra_data_json": "",
    }

    success, stdout, stderr, failure_reason = evaluate_generated_test(
        language,
        generated_test,
        base_path,
        repo_folder_name,
        relative_path
    )

    return {
        "metric": "test",
        "success": success,
        "score": int(success),
        "language": language,
        "reason": failure_reason,
        "extra_data_json": json.dumps({
            "generated_test": generated_test,
            "stderr": stderr,
            "stdout": stdout,
        }),
    }

if __name__ == "__main__":
    # Example usage
    base_path = Path("/mnt/c/users/rahul/test-CEH")
    repo_folder_name = "johanrosenkilde/nasty_python"
    relative_path = Path("main.py")
    language = "python"
    model_response = """
    Here is a test case for the function:

    ```python
    def test_example():
        assert 1 == 1
    ```
    """
    
    result = score_test(base_path, repo_folder_name, relative_path, language, model_response)
    print(result)

