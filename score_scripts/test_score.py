import re
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
from plum.environments.py_repo import PythonRepository
from plum.environments.js_repo import JavascriptRepository
from plum.actions.js_actions import JavascriptActions
from plum.actions.py_actions import PythonActions

def get_code_from_outcome(outcome: str, language: str) -> Optional[str]:
    start_marker = f"```{language}"
    end_marker = "```"
    
    start_index = outcome.find(start_marker)
    if start_index == -1:
        start_marker = "```"
        start_index = outcome.find(start_marker)
    
    if start_index == -1:
        return None
    
    end_index = outcome.find(end_marker, start_index + len(start_marker))
    if end_index == -1:
        return None
    
    extracted_text = outcome[start_index + len(start_marker):end_index].strip()
    
    if language in ("javascript", "typescript"):
        lines = extracted_text.split("\n")
        filtered_lines = [line for line in lines if not line.startswith("import ") and "require(" not in line]
        extracted_text = "\n".join(filtered_lines)
    
    return extracted_text

def evaluate_generated_test(
    language: str,
    generated_test: str,
    base_path: Path,
    repo_folder_name: str,
    relative_path: Path
) -> Tuple[bool, str, str, Optional[str]]:
    """Evaluate a generated test using PLUM

    This function sets up the appropriate repository environment, saves the generated test,
    executes it, and returns the results.

    Args:
        language (str): The programming language of the test ('python', 'javascript', or 'typescript').
        generated_test (str): The string of the test code generated by the model.
        base_path (Path): The base path of the repository.
        repo_folder_name (str): The name of the repository folder (not including any other parts of the path).
        relative_path (Path): The relative path of the test file within the repository.

    Returns:
        Tuple[bool, str, str, Optional[str]]: A tuple containing:
            - success (bool): Whether the test execution was successful.
            - stdout (str): The standard output of the test execution.
            - stderr (str): The standard error output of the test execution.
            - failure_reason (Optional[str]): The reason for failure, if any.
    """
    print(f"Evaluating generated test for {language} in {base_path} with {repo_folder_name}")
    failure_reason = None
    test_contents_used = generated_test

    if language == "python":
        repo = PythonRepository(base_path, repo_folder_name)
        repo.setup(cleanup=False)
        actions = PythonActions(repo)
        test_file = actions.save_generated_test(generated_test, "CES_generated")
        result = actions.execute_test_file(test_file, timeout=60)
    elif language in ("javascript", "typescript"):
        repo = JavascriptRepository(base_path, repo_folder_name, language=language)
        repo.setup(install_reqs=True, cleanup=False)
        actions = JavascriptActions(repo)
        test_library = repo.test_library or "jest"
        focal_file_path = base_path / Path(repo.internal_repo_path) / Path(relative_path)
        
        if test_library == "mocha":
            imports = "var chai = require('chai')\nvar assert = chai.assert;\nvar expect = chai.expect;" if language == "javascript" else "import { assert, expect, should } from 'chai'\n"
            test_contents_used = f"{imports}\n\n{focal_file_path.read_text()}\n\n{generated_test}"
            result = actions.run_npm_test(relative_path, test_library, timeout=60)
        else:
            # if jest
            test_contents_used = f"{focal_file_path.read_text()}\n\n{generated_test}"
            # TODO: Do we need to have this line here now that this has been implemented in PLUM?
            test_file_name = relative_path.stem + "_CES_Generated_Test.test" + relative_path.suffix
            test_file_path = base_path / Path(repo.internal_repo_path) / relative_path.parent / Path(test_file_name)
            test_file_path.write_text(test_contents_used)
            try:
                result = actions.run_npm_test(test_file_path.relative_to(base_path), test_library, timeout=60)
            finally:
                test_file_path.unlink()
    else:
        return False, "", "", f"Unsupported language: {language}"

    if "status_result" in result and "success" not in result:
        result["success"] = result["status_result"] == "SUCCESS"
    if "success" not in result:
        result["success"] = False

    if not result.get("success"):
        failure_reason = "Test execution failed"

    return result.get("success", False), result.get("stdout", ""), result.get("stderr", ""), failure_reason

def score_test(base_path: Path, relative_path: Path, language: str, model_response: str) -> Dict[str, Any]:
    repo_folder_name = ""

    generated_test = get_code_from_outcome(model_response, language)
    if generated_test is None:
        return {
            "success": False,
            "error": "No test code found in the model response",
            "stdout": "",
            "stderr": "",
        }

    success, stdout, stderr, failure_reason = evaluate_generated_test(
        language,
        generated_test,
        base_path,
        repo_folder_name,
        relative_path
    )

    return {
        "success": success,
        "error": failure_reason,
        "stdout": stdout,
        "stderr": stderr,
    }

if __name__ == "__main__":
    # Example usage
    base_path = Path("/path/to/repository")
    relative_path = Path("test_file")
    language = "python"
    model_response = "model response"
    
    result = score_test(base_path, relative_path, language, model_response)
    print(result)

