import re
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from plum.environments.py_repo import PythonRepository
from plum.environments.js_repo import JavascriptRepository
from plum.actions.js_actions import JavascriptActions
from plum.actions.py_actions import PythonActions
import difflib
from abs_conversation import get_task
from abs_state import get_document_file_path, get_terminal_output, get_active_line, get_anchor_line
from base_evaluator import EvaluationResult, Evaluator
from static_analysis_tools import (
    ToolResult,
    get_supported_tools,
    get_tool_run_fn,
)
from timeout import timeout

def get_code_from_outcome(outcome: str, language: str) -> Optional[str]:
    start_marker = f"```{language}"
    end_marker = "```"
    
    start_index = outcome.find(start_marker)
    if start_index == -1:
        start_marker = "```"
        start_index = outcome.find(start_marker)
    
    if start_index == -1:
        return None
    
    end_index = outcome.find(end_marker, start_index + len(start_marker))
    if end_index == -1:
        return None
    
    extracted_text = outcome[start_index + len(start_marker):end_index].strip()
    return extracted_text

def evaluate_fix_with_tool(
        self,
        tool: str,
        repo_folder: Path,
        before_file_path: Path,
        after_file_contents: str,
    ) -> Tuple[float, str, List[ToolResult], List[ToolResult]]:
        """Evaluate the quality of a fix generated by Copilot.

        Args:
            tool (str): The tool to use for evaluation, e.g. pylint, pyright, tsc

        Returns:
            Tuple[float, str, list, list]: A tuple containing the score, reason, and the before and after errors.
        """

        before_file_contents = before_file_path.read_text()
        after_results = []
        before_results = []
        tool_raw_output = []

        tool_fn = get_tool_run_fn(tool)

        cache_key = (str(repo_folder), tool)
        if cache_key in self.before_fix_evaluation:
            before_results = self.before_fix_evaluation[cache_key]
            print("USED CACHE KEY")
        else:
            before_results, _ = tool_fn(repo_folder)
            self.before_fix_evaluation[cache_key] = before_results

        try:
            before_file_path.write_text(after_file_contents)
            after_results, tool_raw_output = tool_fn(repo_folder)

            # There should never be a case of missing before results, so if it is None or an empty list, then there was a tool failure.
            # We can't tell the difference between a tool failure and all problems fixed for after_results. But since that check comes
            # after the check for before_results, it shouldn't cause an issue in practice even though in theory, it could.
            if not before_results:
                score = 0
                reason = f"{tool} failed to run on before fix file"
            elif after_results is None:
                score = 0
                reason = f"{tool} failed to run on after fix file"
            elif len(before_results) > len(after_results):
                score = 1
                reason = f"({tool}) After fix has fewer errors"
            elif len(before_results) == len(after_results):
                score = 0
                reason = f"({tool}) After fix has the same # of errors"
            else:
                score = 0
                reason = f"({tool}) After fix has more errors"
        except:
            score = 0
            reason = f"({tool}) After fix failed to run"
        finally:
            before_file_path.write_text(before_file_contents)

        return score, reason, before_results or [], after_results or [tool_raw_output]

def score_fix(base_path: Path, relative_path: Path, language: str, model_response: str, task: str) -> Dict[str, Any]:
    repo_folder_name = ""
    working_dir = base_path

    generated_fix = get_code_from_outcome(model_response, language)
    if generated_fix is None:
        return {
            "success": False,
            "error": "No fix code found in the model response",
            "score": 0,
            "reason": "No fix generated",
        }

    input_source_file_path = working_dir / relative_path
    input_source_file_contents = input_source_file_path.read_text()

    score, reason, before_errors, after_errors = evaluate_fix_with_tool(
        task,
        working_dir,
        input_source_file_path,
        generated_fix
    )

    unidiff = "\n".join(
        list(
            difflib.unified_diff(
                input_source_file_contents.splitlines(),
                generated_fix.splitlines(),
                fromfile="before",
                tofile="after",
            )
        )
    )

    return {
        "success": score > 0,
        "score": score,
        "reason": reason,
        "errors_before": before_errors,
        "errors_after": after_errors,
        "unidiff": unidiff,
    }

if __name__ == "__main__":
    # Example usage
    base_path = Path("/path/to/repository")
    relative_path = Path("src/main.py")
    language = "python"
    task = "pylint"
    model_response = "Model Response"
    
    result = score_fix(base_path, relative_path, language, model_response, task)
    print(result)

