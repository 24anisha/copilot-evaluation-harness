import json
from pathlib import Path
from typing import Dict, Any, List, Tuple
import difflib
from plum.environments import Repository
from plum.utils import helpers
from score_scripts.static_analysis_tools import (
    ToolResult,
    get_supported_tools,
    get_tool_run_fn,
)

def evaluate_fix_with_tool(
        tool: str,
        repo_folder: Path,
        before_file_path: Path,
        after_file_contents: str,
    ) -> Tuple[float, str, List[ToolResult], List[ToolResult]]:
        """Evaluate the quality of a fix generated by a specified tool.

        Args:
            tool (str): The name of the tool to use for evaluation (e.g., 'pylint', 'pyright', 'tsc').
            repo_folder (Path): The path to the repository folder where the tool will be run.
            before_file_path (Path): The path to the file before the fix is applied.
            after_file_contents (str): The contents of the file after the fix is applied.

        Returns:
            Tuple[float, str, List[ToolResult], List[ToolResult]]: A tuple containing:
                - score (float): The evaluation score based on the number of errors before and after the fix.
                - reason (str): A message explaining the score.
                - before_results (List[ToolResult]): The results from the tool run on the before fix file.
                - after_results (List[ToolResult]): The results from the tool run on the after fix file.
        """
        before_fix_evaluation = {}
        before_file_contents = before_file_path.read_text()
        after_results = []
        before_results = []
        tool_raw_output = []

        tool_fn = get_tool_run_fn(tool)

        cache_key = (str(repo_folder), tool)
        if cache_key in before_fix_evaluation:
            before_results = before_fix_evaluation[cache_key]
            print("USED CACHE KEY")
        else:
            before_results, _ = tool_fn(repo_folder)
            before_fix_evaluation[cache_key] = before_results

        try:
            before_file_path.write_text(after_file_contents)
            after_results, tool_raw_output = tool_fn(repo_folder)

            # There should never be a case of missing before results, so if it is None or an empty list, then there was a tool failure.
            # We can't tell the difference between a tool failure and all problems fixed for after_results. But since that check comes
            # after the check for before_results, it shouldn't cause an issue in practice even though in theory, it could.
            if not before_results:
                score = 0
                reason = f"{tool} failed to run on before fix file"
            elif after_results is None:
                score = 0
                reason = f"{tool} failed to run on after fix file"
            elif len(before_results) > len(after_results):
                score = 1
                reason = f"({tool}) After fix has fewer errors"
            elif len(before_results) == len(after_results):
                score = 0
                reason = f"({tool}) After fix has the same # of errors"
            else:
                score = 0
                reason = f"({tool}) After fix has more errors"
        except:
            score = 0
            reason = f"({tool}) After fix failed to run"
        finally:
            before_file_path.write_text(before_file_contents)

        return score, reason, before_results or [], after_results or [tool_raw_output]

def score_fix(base_path: Path, repo_name: str, relative_path: Path, model_response: str, task: str, language: str) -> Dict[str, Any]:
    """Score the effectiveness of a fix applied to a source file using a specified static analysis tool.

    This function evaluates the quality of a fix by running a static analysis tool on the original and modified
    versions of a source file. It returns a score indicating the success of the fix along with relevant information.

    Args:
        base_path (Path): The base path of the repository.
        repo_name (str): The name of the repository being evaluated.
        relative_path (Path): The relative path to the source file within the repository.
        model_response (str): The contents of the file after the fix is applied.
        task (str): The name of the static analysis tool to use for evaluation (e.g., 'pylint').
        language (str): The programming language of the source file.

    Returns:
        Dict[str, Any]: A dictionary containing the evaluation results, including:
            - metric (str): The type of metric being evaluated (always "fix").
            - success (bool): Indicates whether the fix was successful (score > 0).
            - score (float): The evaluation score based on the number of errors before and after the fix.
            - language (str): The programming language of the source file.
            - reason (str): A message explaining the score.
            - extra_data_json (str): A JSON string containing additional data, such as the unified diff.
    """
    
    working_dir = base_path

    if task not in get_supported_tools():
        return {
            "metric": "fix",
            "success": False,
            "score": 0,
            "language": language,
            "reason": "tool is not supported",
            "original_file_syntax_pass": "",
            "post_file_syntax_pass": "",
            "extra_data_json": ""
        }
    
    repo = Repository(language, base_path, repo_name)
    repo.setup(install_reqs=False)
    repo_folder = working_dir / repo_name.replace('/', '--')

    # repo_folder = working_dir / repo_name.split('/')[1]
    # github_url = "https://github.com/" + repo_name + ".git"
    # helpers.clone_repository(github_url, repo_folder)

    input_source_file_path = repo_folder / relative_path
    input_source_file_contents = input_source_file_path.read_text()

    score, reason, before_errors, after_errors = evaluate_fix_with_tool(
        tool=task,
        repo_folder=repo_folder,
        before_file_path=input_source_file_path,
        after_file_contents=model_response
    )

    repo.cleanup()

    unidiff = "\n".join(
        list(
            difflib.unified_diff(
                input_source_file_contents.splitlines(),
                model_response.splitlines(),
                fromfile="before",
                tofile="after",
            )
        )
    )

    return {
        "metric": "fix",
        "success": score > 0,
        "score": score,
        "language": language,
        "reason": reason,
        "extra_data_json": json.dumps(
            {
                "unidiff": unidiff,
                "original_file_syntax_pass": str(before_errors),
                "post_file_syntax_pass": str(after_errors),
            }
        )
    }

if __name__ == "__main__":
    # Example usage
    base_path = Path("/mnt/c/users/rahul/test-CEH")
    repo_name = "johanrosenkilde/nasty_python"
    relative_path = Path("main.py")
    task = "pylint"
    language = "python"
    model_response = ""
    
    result = score_fix(base_path, repo_name, relative_path, model_response, task, language)
    print(result)